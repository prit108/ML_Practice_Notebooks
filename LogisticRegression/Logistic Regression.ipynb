{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression using Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa', 'versicolor', 'virginica']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "dataset = load_iris()\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "\n",
    "target_names = list(dataset.target_names)\n",
    "print(target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change to binary class\n",
    "y = (y > 0).astype(int)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogReg:\n",
    "    \"\"\"\n",
    "    This implementation of Logistic Regression uses batch gradient descent without regularization.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_iters=300, tolerance = 1e-10, alpha=0.001, threshold=0.5, verbose=False):\n",
    "        self.num_iters = num_iters\n",
    "        self.alpha = alpha\n",
    "        self.tolerance = tolerance\n",
    "        self.threshold = threshold\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def add_ones(self, X):\n",
    "        return np.concatenate((np.ones((len(X),1)), X), axis = 1)\n",
    "      \n",
    "    def sigmoid(self, X, theta):\n",
    "        return 1/(1 + np.exp(X@theta))\n",
    "    \n",
    "    def cost(self, X, y_true):\n",
    "        m = X.shape[0]\n",
    "        y_hat = self.sigmoid(X, self.theta)\n",
    "        \n",
    "        C = np.sum(-1*y_true*np.log(y_hat)-(1-y_true)*np.log(1-y_hat))\n",
    "        \n",
    "        return C\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X = X.copy()\n",
    "        X = self.add_ones(X)\n",
    "        y = y.reshape(-1, 1)\n",
    "        \n",
    "        self.theta = np.zeros((len(X[0]), 1))\n",
    "        \n",
    "        current_iter = 1\n",
    "        norm = 1\n",
    "        while (norm >= self.tolerance and current_iter < self.num_iters):\n",
    "            old_theta = self.theta.copy()\n",
    "            #grad = np.dot(np.transpose(y_hat-self.y), self.X)\n",
    "            grad = X.T@(y - self.sigmoid(X, self.theta))\n",
    "            grad= grad.reshape(-1, 1)\n",
    "            \n",
    "            self.theta = self.theta - self.alpha*grad\n",
    "            \n",
    "            if self.verbose and (current_iter%10 == 0):\n",
    "                print(f'cost for {current_iter} iteration : {self.cost(X, y)}')\n",
    "            norm = np.linalg.norm(old_theta - self.theta)\n",
    "            current_iter += 1\n",
    "            \n",
    "        return self.theta\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        \"\"\"\n",
    "        Returns mse loss for a dataset evaluated on the hypothesis\n",
    "        \"\"\"\n",
    "        X = self.add_ones(X)\n",
    "        return self.cost(X, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        prob = self.predict_proba(X)\n",
    "        return (prob >= self.threshold).astype(int)\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Returns probability of predictions.\n",
    "        \"\"\"\n",
    "        X = self.add_ones(X)  \n",
    "        return self.sigmoid(X, self.theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogReg(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost for 10 iteration : 38.237943342797365\n",
      "cost for 20 iteration : 23.648289147818435\n",
      "cost for 30 iteration : 17.093773043396805\n",
      "cost for 40 iteration : 13.417545508656572\n",
      "cost for 50 iteration : 11.069999125526003\n",
      "cost for 60 iteration : 9.440675928516544\n",
      "cost for 70 iteration : 8.242753020967829\n",
      "cost for 80 iteration : 7.3241220174851716\n",
      "cost for 90 iteration : 6.596706171880882\n",
      "cost for 100 iteration : 6.005993332293331\n",
      "cost for 110 iteration : 5.5164378067658895\n",
      "cost for 120 iteration : 5.103867750598035\n",
      "cost for 130 iteration : 4.751268962496235\n",
      "cost for 140 iteration : 4.44631496738149\n",
      "cost for 150 iteration : 4.179854212480697\n",
      "cost for 160 iteration : 3.9449479301937878\n",
      "cost for 170 iteration : 3.736238156896925\n",
      "cost for 180 iteration : 3.549520845197655\n",
      "cost for 190 iteration : 3.381450370916611\n",
      "cost for 200 iteration : 3.229330528807089\n",
      "cost for 210 iteration : 3.090963838304085\n",
      "cost for 220 iteration : 2.964541009635856\n",
      "cost for 230 iteration : 2.848558604604976\n",
      "cost for 240 iteration : 2.7417568363186846\n",
      "cost for 250 iteration : 2.643071980826401\n",
      "cost for 260 iteration : 2.5515995427840403\n",
      "cost for 270 iteration : 2.466565439764731\n",
      "cost for 280 iteration : 2.3873032376560905\n",
      "cost for 290 iteration : 2.313236003052214\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.27989804],\n",
       "       [ 0.43872127],\n",
       "       [ 1.4921658 ],\n",
       "       [-2.34202659],\n",
       "       [-1.05795168]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = logreg.predict(X)\n",
    "predictions = predictions.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y == predictions) / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
